---
title: "Outliers"
output:             
  html_document:
    theme: united
    toc: yes
---

```{r setup, include=FALSE}
# Configuration locale du chemin du projet SIWIM :
knitr::opts_knit$set(root.dir = normalizePath("C:/Users/schmidt/Documents/GitHub/SiWIM-project/")) 
options(encoding = 'UTF-8')
```
Les valeurs aberrantes peuvent survenir pour diverses raisons, dont la plus évidente est lors d’erreurs de mesure, erreurs de transcription des données, etc.

Cependant, les valeurs aberrantes ne sont pas forcément erronées.  Parfois, elles sont des valeurs qui révèlent un phénomène particulier qui peut être différent du modèle suivi par la majorité des observations.

<!-- install.packages("data.table") -->
<!-- install.packages("dplyr") -->
<!-- install.packages("caret", -->
<!--                  repos = "http://cran.r-project.org",  -->
<!--                  dependencies = TRUE) -->
<!-- install.packages("ggplot2") -->

```{r}
library(data.table)
library(caret)
library(ggplot2)
library(dplyr)

don <- read.csv('2_Data/2_Retraitees/SiWIM_data_after_input_clusters.csv',
                header=T)
dim(don)

names(don)
```

# Méthodologie

Les variables en notre possession sont: 
+ Des paramètres physiques du véhicule: poids total, poids sur essieux, distances entre essieux, 
+ Des variables d'environnement: date et heure de passage, température, 
+ Des variables liées au passage du véhicule: vitesse, voie de passage, 
+ Des variables liées au traitement mathématique et numérique du problème inverse: la mesure du chi2 entre mesure et calcul, différents "warnings" tags quant aux traitements réalisés.

Nous allons utiliser une approche univariée puis une approche multivariée pour déterminer les outliers. 

L'approche univariée, comme son nom l'indique, recherche les outliers par rapport à une seule variable. Une observation qui est inconditionnelle non usuelle est appelée "univariate outlier", donnée aberrante univariée.  

L'approche multivariée passe par la recherche d'un modèle entre les variables en présence. Les outliers sont alors déterminés avec différentes distances (Cook, residual, r-strudent, les hatvalues, Mahalanobis, DFFITS), voir (Outlier detection in multivariate data, K. Senthamarai, K. Manoj, Applied Mathematical Sciences, 2015). 
Une donnée aberrante multivariée est une donnée qui a une valeur non usuelle de la variable $Y$, conditionnellement à la valeur de la/des variable(s) $X$ ($X_i$).

A noter qu'il existe différents packages qui permettent de traiter le problème des outliers (car, outlierD, ...), mais nous avons poréféré ici passer par la recherche de modèles. 


# Approche univariée

Tout d'abord nous créons le dataframe correspondant où les variables manquantes (typiquement les poids des essieux non existants) sont ramenées à 0: 

```{r}
don <- don[,c("Warning_flags","A1","A2","A3","A4","A5","A6","A7",
                      "M1","M2","M3","M4","M5","M6","M7","M8","N", 
                      "Reduced_chi_squared", "Vitesse", "MGV", "clusters_kmeans")]

dim(don)

names(don)

don[is.na(don)] <- 0
```

Un outlier est un individu dont la variable étudiée est en dehors de 1.5*IQR, avec IQR (Inter Quartile Range) est la différence entre le 75\ieme et 25\ieme quantile. 

Cette recherche de outliers peut se faire en univariée à l'aide de bowplot. 

Les sorties de la fonction boxplot sont: 
+ stats: vecteur de longueur 5, qui contient la valeur extrême de la moustache inférieure, la charnière inférieure, la médiane, la charbnière supérieure et la valeur extrême de la moustache supérieure. 
+ n: le nombre d'observations NA dans l'échantillon.	
+ conf: les valeurs inférieures et supérieures du ‘notch’ (if(do.conf)). 
+ out : les valeurs des points en dehors des extrêmes des moustaches. 

Ici nous effectuons ce test pour différentes variables puis créons une colonne supplémentaire dans nos données qui identifie l'individu comme un outlier ("TRUE") ou pas("FALSE). 

## Pour le poids total en charge

```{r}
outlier_values <- boxplot.stats(don$MGV)$out
boxplot(don$MGV, main="Poids total", boxwex=0.1)
length(outlier_values) # 142 outliers

check_outlier <- function(v, coef=1.5){
  quantiles <- quantile(v,probs=c(0.25,0.75))
  IQR <- quantiles[2]-quantiles[1]
  res <- v < (quantiles[1]-coef*IQR)|v > (quantiles[2]+coef*IQR)
  return(res)
}

don$outlier <- check_outlier(don$MGV)

# write.csv(don, file = "../../2_Data/2_Retraitees/SiWIM_data_after_input_clusters_outliers.csv")
```
```{r}
ggplot(don,aes(x=MGV,y=Reduced_chi_squared, colour=outlier))+
  geom_boxplot()+
  geom_text(aes(label=Warning_flags),hjust=-0.3)
```

```{r}
ggplot(don,aes(x=outlier,y=Reduced_chi_squared, colour=Warning_flags))+
   geom_boxplot()
```


```{r}
ggplot(don, aes(y=Warning_flags, x=Reduced_chi_squared, group=outlier)) +
  geom_point(aes(shape=outlier, color=outlier))
```

## Pour le poids sur essieu 2

```{r}
don$outlier <- check_outlier(don$M2)
ggplot(don, aes(y=Warning_flags, x=Reduced_chi_squared, group=outlier)) +
  geom_point(aes(shape=outlier, color=outlier))
```


## Pour la vitesse

```{r}
don$outlier <- check_outlier(don$Vitesse)
ggplot(don, aes(y=Warning_flags, x=Reduced_chi_squared, group=outlier)) +
  geom_point(aes(shape=outlier, color=outlier))
```


# Approche multivariée

Pour l'approche multivariée: 
+ On cherche un modèle représentant les données, 
+ puis avec ce modèle on définit les outliers comme les individus ayant une distance de Cook supérieure à 4 fois la distance de Cook moyenne. 

La valeur 4 n'est pas une valeur fixe ou définitive. 

De plus, d'autres approches peuvent être mises en place, par exemple avec les résidues, les hat values et les valeurs de r-student. 

## Recherche du modèle avec caret

```{r}
ptm <- proc.time()

train <- don[1:1000,c("A1","A2","A3","A4","A5","A6","A7",
                 "M1","M2","M3","M4","M5","M6","M7","M8","N", 
                 "Vitesse", "MGV", "Reduced_chi_squared")]

str(train)

sum(is.na(train))
```

On impute les valuers manquantes (NAN) en utilisant méthode KNN. 
De plus, les données sont centrées et réduites avant de rechercher le modèle. 

```{r}
preProcValues <- caret::preProcess(train, method = c("knnImpute","center","scale"))
sum(is.na(train))


library('RANN')
train_processed <- predict(preProcValues, train)
sum(is.na(train_processed))

str(train_processed)
```

L'échantillon de données est coupé en deux, pour la pouplation d'apprentissage (75%) et celle de test (25%). 
Cela est fait facilement avec la fonction createDataPartition du package caret.

```{r}
index <- createDataPartition(train_processed$MGV, p=0.75, list=FALSE)
trainSet <- train_processed[ index,]
testSet <- train_processed[-index,]

str(trainSet)
```

On utilise la fonction rfe pour contrôler les features (ici validation croisée, avec 3 répétitions). 

De plus, le calage de modèle est aussi contrôlée par validation croisée, avec 5 répétitions.

```{r}
control <- rfeControl(functions = rfFuncs,
                      method = "repeatedcv",
                      repeats = 3,
                      verbose = FALSE)
outcomeName<-'MGV'
predictors<-names(trainSet)[!names(trainSet) %in% outcomeName]
MGV_Pred_Profile <- rfe(trainSet[,predictors], trainSet[,outcomeName],
                         rfeControl = control)
MGV_Pred_Profile

names(getModelInfo())

fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5)
```

Les modèles recherchés sont le modèle linéaire généralisé, celui avec pénalisation Lasso, les forêts aléatoires, et les gradient boosting. 

## Modèle linéaire généralisé

```{r}
model_glm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',trControl=fitControl,tuneLength=10)
print(model_glm)
plot(model_glm)
```

## Modèle linéaire généralisé avec pénalisation Lasso

```{r}
model_rqlasso<-train(trainSet[,predictors],trainSet[,outcomeName],method='rqlasso',
                     ,trControl=fitControl,tuneLength=10)
print(model_rqlasso)
plot(model_rqlasso)
```

## Forêts aléatoires

```{r}
model_rf<-train(trainSet[,predictors],trainSet[,outcomeName],method='rf',
                ,trControl=fitControl,tuneLength=10) # random Forest
print(model_rf)
plot(model_rf)
```

## Gradient boosting

```{r}
model_gbm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',
                 ,trControl=fitControl,tuneLength=10) #Stochastic Gradient Boosting
print(model_gbm)
plot(model_gbm)
```

## Comparaison des différents modèles

```{r}
results <- resamples(list(mod.glm=model_glm, 
                          mod.rqlasso=model_rqlasso, 
                          mod.rf=model_rf,
                          mod.rf=model_rf, mod.gbm=model_gbm))
results$values
summary(results)
bwplot(results)
bwplot(results)
dotplot(results)

CPU_time <- proc.time() - ptm
print(CPU_time)
```


## Recherche des outliers

Nous recherchons le modèle linéaire généralisé avec toutes les données, et en utilisant la fonction glm de R base. 

```{r}
rm(list=ls())
don <- read.csv('2_Data/2_Retraitees/SiWIM_data_after_input_clusters.csv',header=T)
dim(don)
model_glm_glm <- glm(MGV~., data=don, family="gaussian")
```

### Avec les résidus

Une règle approximative consiste à dire que les données aberrantes sont les individus avec un résidus standardisés supérieurs à 3.3 (correspondant à un niveau de $\alpha$ de .001).

```{r}
resid_glm <- tail(resid(model_glm_glm),n=1000)

plot(resid_glm, pch="*", cex=2, 
     main="Observations influentes, avec les résidus") 

influential <- as.numeric(names(resid_glm)) # Lignes influentes
head(train[influential, ])

train$outlier <- "FALSE"
train[influential,"outlier"] <- "TRUE"

ggplot(train, aes(y=Warning_flags, x=Reduced_chi_squared, group=outlier)) +
  geom_point(aes(shape=outlier, color=outlier))
```
###Avec la distance de Cook 

La distance de Cook est essentiellement une mesure de distance standardisée qui permet de décrire le changement dans l’estimateur de $\beta$ lorsque l’on retire l’observation $i$.
Une grande valeur de la distance de Cook suggère que l’observation i possède une grande influence.

```{r}
cooksd <- cooks.distance(model_glm_glm)

plot(cooksd, pch="*", cex=2, 
     main="Observations influentes, avec la distance de Cook") 
abline(h = 4*mean(cooksd, na.rm=T), col="red")  
text(x=1:length(cooksd)+1, y=cooksd, 
     labels=ifelse(cooksd>4*mean(cooksd, na.rm=T),names(cooksd),""), 
     col="red") 

influential <- names(cooksd)[(cooksd > 4*mean(cooksd, na.rm=T))] # Lignes influentes
head(train[influential, ])

train$outlier <- "FALSE"
train[influential,"outlier"] <- "TRUE"

ggplot(train, aes(y=Warning_flags, x=Reduced_chi_squared, group=outlier)) +
  geom_point(aes(shape=outlier, color=outlier))
```


### Avec les hat values

La statistique de leverage (leverage points = points influents), $h$, aussi appelée hat value (issue de la hat matrice, aussi appelée matrice de projection), identfie les individus qui influencent plus la line de régression que les autres.

```{r}
hat_values <- tail(hatvalues(model_glm_glm),n=1000)

plot(hat_values, pch="*", cex=2, 
     main="Observations influentes, avec les hat values") 

influential <- as.numeric(names(hat_values)) # Lignes influentes
head(train[influential, ])

train$outlier <- "FALSE"
train[influential,"outlier"] <- "TRUE"

ggplot(train, aes(y=Warning_flags, x=Reduced_chi_squared, group=outlier)) +
  geom_point(aes(shape=outlier, color=outlier))
```

### Avec les résidus de r-student

Un résidu studentisé est le résidu observé divisé par l'écart-type. 

```{r}
r_stud <- tail(rstudent(model_glm_glm),n=1000)

plot(r_stud, pch="*", cex=2, 
     main="Observations influentes, avec les résidus") 

influential <- as.numeric(names(r_stud)) # Lignes influentes
head(train[influential, ])

train$outlier <- "FALSE"
train[influential,"outlier"] <- "TRUE"

ggplot(train, aes(y=Warning_flags, x=Reduced_chi_squared, group=outlier)) +
  geom_point(aes(shape=outlier, color=outlier))
```

# Avec le package car

Le package car apporte des fonctions pour rechercher directement les outliers, et selon différentes méthodes (avec la distance de Cook, les hat values, les valeurs R-student).

```{r}
library(car)

outs <- influencePlot(model_glm_glm)

n <- 1000
Cooksdist <- as.numeric(tail(row.names(outs[order(outs$CookD), ]), n))
Lev <- as.numeric(tail(row.names(outs[order(outs$Hat), ]), n))
StdRes <- as.numeric(tail(row.names(outs[order(outs$StudRes), ]), n))

plot(train$Reduced_chi_squared, train$Warning_flags)
points(train$Reduced_chi_squared[Cooksdist], train$Warning_flags[Cooksdist], col = "red", pch = 0, lwd = 15)
points(train$Reduced_chi_squared[Lev], train$Warning_flags[Lev], col = "blue", pch = 25, lwd = 8)
points(train$Reduced_chi_squared[StdRes], train$Warning_flags[StdRes], col = "green", pch = 20, lwd = 5)
```






